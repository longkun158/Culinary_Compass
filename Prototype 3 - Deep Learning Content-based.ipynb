{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "049c0110-7c37-4b17-9c0f-10e2fd12c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv\n",
    "from torch_geometric.transforms import ToUndirected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a07ed8-e14a-444f-a55c-d33714b475b4",
   "metadata": {},
   "source": [
    "## Load and Pre-process Data (IPCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc41a1d8-6fc0-4be5-8621-e66588745de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dish Count: 518\n",
      "Cleaned Dish Count: 518\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# 1. LOAD & PREPROCESS DATA\n",
    "###############################\n",
    "\n",
    "# Paths to your data files (adjust as needed)\n",
    "flavor_embedded_path = \"flavor_embedded.pkl\"\n",
    "menu_embedded_path = \"menu_embedded.pkl\"\n",
    "user_dataset_path = \"user_dataset_updated.csv\"\n",
    "new_nodes_path = \"new_nodes.csv\"  # Comprehensive list of ingredients\n",
    "\n",
    "# Load pickle and CSV files\n",
    "with open(flavor_embedded_path, \"rb\") as f:\n",
    "    flavor_embedded = pickle.load(f)\n",
    "with open(menu_embedded_path, \"rb\") as f:\n",
    "    menu_embedded = pickle.load(f)\n",
    "\n",
    "user_df = pd.read_csv(user_dataset_path)\n",
    "new_nodes = pd.read_csv(new_nodes_path)\n",
    "\n",
    "# ✅ Remove dishes with missing embeddings\n",
    "menu_embedded = menu_embedded[menu_embedded[\"Embeddings\"].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# ✅ Verify the number of dishes after cleaning\n",
    "print(f\"Original Dish Count: {menu_embedded.shape[0]}\")\n",
    "print(f\"Cleaned Dish Count: {menu_embedded_cleaned.shape[0]}\")\n",
    "\n",
    "# Define the column that holds the raw embedding lists\n",
    "embedding_col = \"Embeddings\"\n",
    "\n",
    "# Function to aggregate embeddings via IPCA ensuring a 300-dim output\n",
    "def aggregate_embeddings_ipca(embedding_list):\n",
    "    if isinstance(embedding_list, list) and len(embedding_list) > 0:\n",
    "        embedding_array = np.array(embedding_list)\n",
    "        if embedding_array.ndim == 2 and embedding_array.shape[0] > 1:\n",
    "            ipca = IncrementalPCA(n_components=1, batch_size=min(embedding_array.shape[0], 50))\n",
    "            ipca_embedding = ipca.fit_transform(embedding_array.T).flatten()\n",
    "            if len(ipca_embedding) < 300:\n",
    "                ipca_embedding = np.pad(ipca_embedding, (0, 300 - len(ipca_embedding)), mode='constant')\n",
    "            return ipca_embedding\n",
    "        elif embedding_array.ndim == 2 and embedding_array.shape[0] == 1:\n",
    "            return embedding_array.flatten()\n",
    "    return np.zeros(300)\n",
    "\n",
    "# Apply the aggregation to menu and flavor datasets\n",
    "menu_embedded[\"Aggregated_Embeddings\"] = menu_embedded[embedding_col].apply(aggregate_embeddings_ipca)\n",
    "flavor_embedded[\"Aggregated_Embeddings\"] = flavor_embedded[embedding_col].apply(aggregate_embeddings_ipca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "982ca963-f206-433a-8a5b-6cca307f6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HeteroData graph\n",
    "data = HeteroData()\n",
    "\n",
    "# --- Dish Nodes ---\n",
    "menu_embedded = menu_embedded.reset_index(drop=True)\n",
    "dish_features = np.stack(menu_embedded[\"Aggregated_Embeddings\"].values)\n",
    "dish_id_to_index = {str(dish_id): idx for idx, dish_id in enumerate(menu_embedded[\"dish_id\"])}\n",
    "\n",
    "# --- Ingredient Nodes ---\n",
    "new_nodes = new_nodes.reset_index(drop=True)\n",
    "ingredient_to_index = {ing: idx for idx, ing in enumerate(new_nodes[\"name\"])}\n",
    "\n",
    "# --- User Nodes ---\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "user_id_to_index = {str(uid): idx for idx, uid in enumerate(sorted(user_df[\"user_id\"].unique()))}\n",
    "\n",
    "\n",
    "# --- User-Dish Edge Assignment ---\n",
    "user_dish_edge_src = []\n",
    "user_dish_edge_dst = []\n",
    "\n",
    "for _, row in user_df.iterrows():\n",
    "    uid = str(row[\"user_id\"])  # Ensure user_id is a string\n",
    "    dish_list = []\n",
    "\n",
    "    # Process 'picked_dishes'\n",
    "    picked = str(row.get(\"picked_dishes\", \"\"))\n",
    "    if picked and picked.lower() != \"nan\":\n",
    "        dish_list.extend([str(d.strip()) for d in picked.split(\",\")])\n",
    "\n",
    "    # Process 'recommended_pick'\n",
    "    rec = str(row.get(\"recommended_pick\", \"\"))\n",
    "    if rec and rec.lower() != \"nan\":\n",
    "        dish_list.extend([str(d.strip()) for d in rec.split(\",\")])\n",
    "\n",
    "    # Assign edges\n",
    "    for d in dish_list:\n",
    "        if d in dish_id_to_index:\n",
    "            user_dish_edge_src.append(user_id_to_index[uid])  # Map user_id to index\n",
    "            user_dish_edge_dst.append(dish_id_to_index[d])    # Map dish_id to index\n",
    "\n",
    "# --- Edges: Dish <-> Ingredient ---\n",
    "dish_ing_edge_src = []\n",
    "dish_ing_edge_dst = []\n",
    "for idx, row in menu_embedded.iterrows():\n",
    "    dish_idx = idx\n",
    "    ing_str = row[\"ingredients_mapped\"]\n",
    "    if pd.notna(ing_str):\n",
    "        ingredients = [ing.strip().lower() for ing in ing_str.split(\",\")]\n",
    "        for ing in ingredients:\n",
    "            if ing in ingredient_to_index:\n",
    "                dish_ing_edge_src.append(dish_idx)\n",
    "                dish_ing_edge_dst.append(ingredient_to_index[ing])\n",
    "\n",
    "# Convert edge lists to tensors\n",
    "data[\"user\", \"interacts\", \"dish\"].edge_index = torch.tensor([user_dish_edge_src, user_dish_edge_dst], dtype=torch.long)\n",
    "data[\"dish\", \"contains\", \"ingredient\"].edge_index = torch.tensor([dish_ing_edge_src, dish_ing_edge_dst], dtype=torch.long)\n",
    "\n",
    "# Ensure the graph is undirected\n",
    "data = ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30ae99d3-ca04-4b31-80fd-617ce396da7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Node Features Shape: torch.Size([10, 300])\n",
      "Dish Node Features Shape: torch.Size([518, 300])\n",
      "Ingredient Node Features Shape: torch.Size([6653, 300])\n"
     ]
    }
   ],
   "source": [
    "# Ensure feature sizes match the number of nodes\n",
    "num_users = len(user_id_to_index)\n",
    "num_dishes = len(dish_id_to_index)\n",
    "num_ingredients = len(ingredient_to_index)\n",
    "\n",
    "# Assign node features\n",
    "# Compute user features as the mean of their interacted dish embeddings\n",
    "user_features = torch.zeros((len(user_id_to_index), 300), dtype=torch.float)  # Placeholder\n",
    "\n",
    "for idx, row in user_df.iterrows():\n",
    "    uid = str(row[\"user_id\"])\n",
    "    dish_list = []\n",
    "\n",
    "    picked = str(row.get(\"picked_dishes\", \"\"))\n",
    "    if picked and picked.lower() != \"nan\":\n",
    "        dish_list.extend([str(d.strip()) for d in picked.split(\",\")])\n",
    "\n",
    "    rec = str(row.get(\"recommended_pick\", \"\"))\n",
    "    if rec and rec.lower() != \"nan\":\n",
    "        dish_list.extend([str(d.strip()) for d in rec.split(\",\")])\n",
    "\n",
    "    # Convert dish embeddings from numpy to tensor\n",
    "    valid_dish_embeddings = [torch.tensor(dish_features[dish_id_to_index[d]], dtype=torch.float) \n",
    "                             for d in dish_list if d in dish_id_to_index]\n",
    "\n",
    "    if valid_dish_embeddings:\n",
    "        user_features[user_id_to_index[uid]] = torch.stack(valid_dish_embeddings).mean(dim=0)\n",
    "\n",
    "# Assign user features to graph\n",
    "data[\"user\"].x = user_features\n",
    "\n",
    "data[\"dish\"].x = torch.tensor(np.stack(menu_embedded[\"Aggregated_Embeddings\"].values), dtype=torch.float)  # Dishes: precomputed embeddings\n",
    "data[\"ingredient\"].x = torch.randn((num_ingredients, 300), dtype=torch.float)  # Ingredients: randomly initialized embeddings\n",
    "\n",
    "# Validate feature shapes\n",
    "print(\"User Node Features Shape:\", data[\"user\"].x.shape)\n",
    "print(\"Dish Node Features Shape:\", data[\"dish\"].x.shape)\n",
    "print(\"Ingredient Node Features Shape:\", data[\"ingredient\"].x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981fce82-4220-41e4-91ed-422a519e2080",
   "metadata": {},
   "source": [
    "## Checking Nodes and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f1ae3aa-b79f-4068-86a8-f801f2df2761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Types: ['user', 'dish', 'ingredient']\n",
      "Edge Types: [('user', 'interacts', 'dish'), ('dish', 'contains', 'ingredient'), ('dish', 'rev_interacts', 'user'), ('ingredient', 'rev_contains', 'dish')]\n",
      "Number of Users: 10\n",
      "Number of Dishes: 518\n",
      "Number of Ingredients: 6653\n",
      "User-Dish Edges: 600\n",
      "Dish-Ingredient Edges: 2656\n",
      "User Node Features Shape: torch.Size([10, 300])\n",
      "Dish Node Features Shape: torch.Size([518, 300])\n",
      "Ingredient Node Features Shape: torch.Size([6653, 300])\n"
     ]
    }
   ],
   "source": [
    "# Print Debug Information Locally\n",
    "print(\"Node Types:\", data.node_types)\n",
    "print(\"Edge Types:\", data.edge_types)\n",
    "print(\"Number of Users:\", len(user_id_to_index))\n",
    "print(\"Number of Dishes:\", len(dish_id_to_index))\n",
    "print(\"Number of Ingredients:\", len(ingredient_to_index))\n",
    "print(\"User-Dish Edges:\", data[\"user\", \"interacts\", \"dish\"].edge_index.shape[1])\n",
    "print(\"Dish-Ingredient Edges:\", data[\"dish\", \"contains\", \"ingredient\"].edge_index.shape[1])\n",
    "print(\"User Node Features Shape:\", data[\"user\"].x.shape)\n",
    "print(\"Dish Node Features Shape:\", data[\"dish\"].x.shape)\n",
    "print(\"Ingredient Node Features Shape:\", data[\"ingredient\"].x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9095e6d9-1028-4d92-aed0-2440f5e2f769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  user={ x=[10, 300] },\n",
      "  dish={ x=[518, 300] },\n",
      "  ingredient={ x=[6653, 300] },\n",
      "  (user, interacts, dish)={ edge_index=[2, 600] },\n",
      "  (dish, contains, ingredient)={ edge_index=[2, 2656] },\n",
      "  (dish, rev_interacts, user)={ edge_index=[2, 600] },\n",
      "  (ingredient, rev_contains, dish)={ edge_index=[2, 2656] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c976bcb6-6cac-466b-bb81-47ea96c322f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes in data: ['user', 'dish', 'ingredient']\n"
     ]
    }
   ],
   "source": [
    "print(\"Nodes in data:\", data.node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12fb0360-5b5b-489f-b75f-c65b5ecd552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge types: [('user', 'interacts', 'dish'), ('dish', 'contains', 'ingredient'), ('dish', 'rev_interacts', 'user'), ('ingredient', 'rev_contains', 'dish')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Edge types:\", data.edge_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8d528-1d7a-4644-a566-cac7920fb784",
   "metadata": {},
   "source": [
    "## Splitting Data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b558ef6-4965-4479-a765-93908e90f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train User-Dish Edges: 480\n",
      "✅ Validation User-Dish Edges: 60\n",
      "✅ Test User-Dish Edges: 60\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Convert edge list to tuples for easier manipulation\n",
    "edges = list(zip(user_dish_edge_src, user_dish_edge_dst))\n",
    "\n",
    "# Step 1: Shuffle the edges randomly\n",
    "random.shuffle(edges)\n",
    "\n",
    "# Step 2: Manually split into 80-10-10\n",
    "num_edges = len(edges)\n",
    "train_split = int(0.8 * num_edges)\n",
    "val_split = int(0.9 * num_edges)  # 80% train, 10% val, 10% test\n",
    "\n",
    "train_edges = edges[:train_split]\n",
    "val_edges = edges[train_split:val_split]\n",
    "test_edges = edges[val_split:]\n",
    "\n",
    "# Step 3: Convert back to tensors\n",
    "train_edge_index = torch.tensor(list(zip(*train_edges)), dtype=torch.long)\n",
    "val_edge_index = torch.tensor(list(zip(*val_edges)), dtype=torch.long)\n",
    "test_edge_index = torch.tensor(list(zip(*test_edges)), dtype=torch.long)\n",
    "\n",
    "# Step 4: Assign edges back to separate datasets\n",
    "train_data = data.clone()\n",
    "train_data[\"user\", \"interacts\", \"dish\"].edge_index = train_edge_index\n",
    "\n",
    "val_data = data.clone()\n",
    "val_data[\"user\", \"interacts\", \"dish\"].edge_index = val_edge_index\n",
    "\n",
    "test_data = data.clone()\n",
    "test_data[\"user\", \"interacts\", \"dish\"].edge_index = test_edge_index\n",
    "\n",
    "# Step 5: Debug print to confirm correct split\n",
    "print(\"✅ Train User-Dish Edges:\", train_data[\"user\", \"interacts\", \"dish\"].edge_index.shape[1])\n",
    "print(\"✅ Validation User-Dish Edges:\", val_data[\"user\", \"interacts\", \"dish\"].edge_index.shape[1])\n",
    "print(\"✅ Test User-Dish Edges:\", test_data[\"user\", \"interacts\", \"dish\"].edge_index.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba26f9-a887-4737-94bd-e03404d38d32",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "97acc644-254c-4614-a56e-2ba7a7a5c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "# ✅ Define GraphSAGE Model with LSTM Aggregation\n",
    "class GraphSAGE_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2):\n",
    "        super(GraphSAGE_LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define GraphSAGE layers with LSTM aggregation\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_dim if i == 0 else hidden_dim\n",
    "            self.convs.append(SAGEConv(in_dim, hidden_dim, aggr=\"lstm\"))  # 🔥 LSTM Aggregation\n",
    "\n",
    "        # Final MLP for link prediction\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)  # Two node embeddings → Single score\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Pass through GraphSAGE layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)  # Activation function\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        # Get embeddings for each node in the edge pairs\n",
    "        z_src = z[edge_index[0]]\n",
    "        z_dst = z[edge_index[1]]\n",
    "\n",
    "        # Concatenate both embeddings and pass through the MLP\n",
    "        pred = torch.sigmoid(self.fc(torch.cat([z_src, z_dst], dim=1)))\n",
    "        return pred.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f7bb508b-6d92-4b76-a94b-1b359ba1c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Edge Index Sorted Successfully!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Sort edge_index by destination node (column 1)\n",
    "def sort_edge_index(edge_index):\n",
    "    sorted_indices = edge_index[1].argsort()  # Sort by destination node\n",
    "    return edge_index[:, sorted_indices]\n",
    "\n",
    "# Apply sorting\n",
    "data[\"user\", \"interacts\", \"dish\"].edge_index = sort_edge_index(data[\"user\", \"interacts\", \"dish\"].edge_index)\n",
    "\n",
    "print(\"✅ Edge Index Sorted Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe133b2-3f65-4c7f-aad7-2a8e8f261a57",
   "metadata": {},
   "source": [
    "## Training and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f84f72f-38ee-4913-8a44-3957424e8155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sorted Train Edge Index: tensor([[ 7,  1,  1,  1,  1,  3,  9,  2,  3,  2],\n",
      "        [ 4,  4,  4,  4,  4,  4, 25, 25, 28, 46]])\n"
     ]
    }
   ],
   "source": [
    "# ✅ Ensure edge_index is sorted by destination nodes (second column)\n",
    "def sort_edge_index(edge_index):\n",
    "    sorted_indices = edge_index[1].argsort()  # Sort by destination node\n",
    "    return edge_index[:, sorted_indices]\n",
    "\n",
    "# Apply sorting to all datasets\n",
    "train_data[\"user\", \"interacts\", \"dish\"].edge_index = sort_edge_index(train_data[\"user\", \"interacts\", \"dish\"].edge_index)\n",
    "val_data[\"user\", \"interacts\", \"dish\"].edge_index = sort_edge_index(val_data[\"user\", \"interacts\", \"dish\"].edge_index)\n",
    "test_data[\"user\", \"interacts\", \"dish\"].edge_index = sort_edge_index(test_data[\"user\", \"interacts\", \"dish\"].edge_index)\n",
    "\n",
    "# Debug: Check sorting correctness\n",
    "print(\"✅ Sorted Train Edge Index:\", train_data[\"user\", \"interacts\", \"dish\"].edge_index[:, :10])  # Print first 10 sorted edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f61fcda8-4de3-4d75-a6af-4756ef4a81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "# ✅ Training Function for GraphSAGE\n",
    "def train(model, data, optimizer, criterion, num_epochs=20):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract node features for each type\n",
    "        user_x = data[\"user\"].x\n",
    "        dish_x = data[\"dish\"].x\n",
    "        ingredient_x = data[\"ingredient\"].x\n",
    "\n",
    "        # Combine into a single tensor (order must match edge_index)\n",
    "        x = torch.cat([user_x, dish_x, ingredient_x], dim=0)\n",
    "\n",
    "        # Forward pass\n",
    "        z = model(x, data[\"user\", \"interacts\", \"dish\"].edge_index)  # Node embeddings\n",
    "        pred = model.decode(z, data[\"user\", \"interacts\", \"dish\"].edge_index)  # Edge predictions\n",
    "\n",
    "        # Binary labels (all edges in train_data are positive)\n",
    "        labels = torch.ones(pred.shape, dtype=torch.float)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2699b76a-c417-487f-9bae-ed824a843842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.6793\n",
      "Epoch 5 | Loss: 0.0000\n",
      "Epoch 10 | Loss: 0.0000\n",
      "Epoch 15 | Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Define model, loss, and optimizer\n",
    "input_dim = 300  # Same as node feature size\n",
    "hidden_dim = 128  # Hidden layer size\n",
    "num_layers = 2\n",
    "\n",
    "model = GraphSAGE_LSTM(input_dim, hidden_dim, num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "# Train the model\n",
    "model = train(model, train_data, optimizer, criterion, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32d1d2e2-3a4b-4485-b144-5e2e7e9aa18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.5000 | F1 Score: 0.6667\n",
      "AUC-ROC: 0.5000 | F1 Score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "# ✅ Generate negative samples\n",
    "def sample_negative_edges(data, num_neg_samples=None):\n",
    "    user_ids = list(range(data[\"user\"].x.shape[0]))  # Get user indices\n",
    "    dish_ids = list(range(data[\"dish\"].x.shape[0]))  # Get dish indices\n",
    "    \n",
    "    pos_edges = set(zip(\n",
    "        data[\"user\", \"interacts\", \"dish\"].edge_index[0].tolist(),\n",
    "        data[\"user\", \"interacts\", \"dish\"].edge_index[1].tolist()\n",
    "    ))\n",
    "\n",
    "    neg_edges = set()\n",
    "    num_neg_samples = num_neg_samples or len(pos_edges)  # Same number of negatives as positives\n",
    "\n",
    "    while len(neg_edges) < num_neg_samples:\n",
    "        u = random.choice(user_ids)\n",
    "        d = random.choice(dish_ids)\n",
    "        if (u, d) not in pos_edges:\n",
    "            neg_edges.add((u, d))\n",
    "    \n",
    "    return list(neg_edges)\n",
    "\n",
    "# ✅ Updated Evaluation Function\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract node features\n",
    "        user_x = data[\"user\"].x\n",
    "        dish_x = data[\"dish\"].x\n",
    "        ingredient_x = data[\"ingredient\"].x\n",
    "\n",
    "        # Combine all node features into a single tensor\n",
    "        x = torch.cat([user_x, dish_x, ingredient_x], dim=0)\n",
    "\n",
    "        # Get positive edges\n",
    "        pos_edges = data[\"user\", \"interacts\", \"dish\"].edge_index\n",
    "\n",
    "        # Get negative edges\n",
    "        neg_edges = sample_negative_edges(data, num_neg_samples=pos_edges.shape[1])\n",
    "        neg_edges = torch.tensor(list(zip(*neg_edges)), dtype=torch.long)\n",
    "\n",
    "        # Combine positive & negative edges\n",
    "        all_edges = torch.cat([pos_edges, neg_edges], dim=1)\n",
    "\n",
    "        # Predict probabilities for all edges\n",
    "        z = model(x, data[\"user\", \"interacts\", \"dish\"].edge_index)\n",
    "        pred = model.decode(z, all_edges)\n",
    "\n",
    "        # Create labels (1 for positive edges, 0 for negative edges)\n",
    "        labels = torch.cat([torch.ones(pos_edges.shape[1]), torch.zeros(neg_edges.shape[1])], dim=0)\n",
    "\n",
    "        # Convert predictions to binary\n",
    "        pred_binary = (pred > 0.5).float()\n",
    "\n",
    "        # Compute metrics\n",
    "        auc = roc_auc_score(labels.cpu().numpy(), pred.cpu().numpy())\n",
    "        f1 = f1_score(labels.cpu().numpy(), pred_binary.cpu().numpy())\n",
    "\n",
    "        print(f\"AUC-ROC: {auc:.4f} | F1 Score: {f1:.4f}\")\n",
    "\n",
    "# ✅ Evaluate on validation set\n",
    "evaluate(model, val_data)\n",
    "\n",
    "# ✅ Evaluate on test set\n",
    "evaluate(model, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0082a95d-bab7-4ded-996a-c21a1112c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Feature Mean: 0.0005289397086016834\n",
      "Dish Feature Mean: 0.00021455016394611448\n",
      "Ingredient Feature Mean: -0.0004126960993744433\n"
     ]
    }
   ],
   "source": [
    "print(\"User Feature Mean:\", train_data[\"user\"].x.mean().item())\n",
    "print(\"Dish Feature Mean:\", train_data[\"dish\"].x.mean().item())\n",
    "print(\"Ingredient Feature Mean:\", train_data[\"ingredient\"].x.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f30443c8-b201-466a-9b4a-cf58d8ae86d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Feature Variance: 0.050580669194459915\n",
      "Dish Feature Variance: 0.05679517984390259\n",
      "Ingredient Feature Variance: 0.9975167512893677\n",
      "Sample User Embeddings: tensor([[-0.3818, -0.1812, -0.0566,  ..., -0.1910, -0.1881, -0.1661],\n",
      "        [-0.3129, -0.1601, -0.1108,  ..., -0.1060, -0.1099, -0.2797],\n",
      "        [-0.3615, -0.2108, -0.2582,  ..., -0.1138, -0.3502, -0.0794],\n",
      "        [-0.3597, -0.1699, -0.0604,  ..., -0.1998, -0.1425, -0.1612],\n",
      "        [-0.4707, -0.1408, -0.1596,  ..., -0.1332, -0.1623, -0.0121]])\n",
      "Sample Dish Embeddings: tensor([[-0.3295,  0.2405,  0.0800,  ..., -0.3857, -0.2247, -0.0543],\n",
      "        [-0.1140, -0.0286, -0.4295,  ..., -0.0589, -0.0998, -0.2829],\n",
      "        [-0.3558, -0.1527, -0.1033,  ..., -0.1148, -0.1160, -0.3739],\n",
      "        [-0.5157,  0.1573,  0.2411,  ..., -0.2855, -0.0706, -0.1868],\n",
      "        [-0.1648, -0.3328, -0.3008,  ..., -0.5351, -0.3513,  0.0387]])\n"
     ]
    }
   ],
   "source": [
    "# Check variance of user, dish, and ingredient features\n",
    "print(\"User Feature Variance:\", train_data[\"user\"].x.var().item())\n",
    "print(\"Dish Feature Variance:\", train_data[\"dish\"].x.var().item())\n",
    "print(\"Ingredient Feature Variance:\", train_data[\"ingredient\"].x.var().item())\n",
    "\n",
    "# Check first 5 user embeddings\n",
    "print(\"Sample User Embeddings:\", train_data[\"user\"].x[:5])\n",
    "\n",
    "# Check first 5 dish embeddings\n",
    "print(\"Sample Dish Embeddings:\", train_data[\"dish\"].x[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "15df6c84-cf70-4b4c-a001-d8f2f567f0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected Dishes: 68/518 (13.13%)\n",
      "⚠️ Unconnected Dishes: 450/518 (86.87%)\n"
     ]
    }
   ],
   "source": [
    "# Count total dishes\n",
    "total_dishes = len(dish_id_to_index)\n",
    "\n",
    "# Count connected dishes\n",
    "connected_dishes = len(set(data[\"user\", \"interacts\", \"dish\"].edge_index[1].tolist()))\n",
    "\n",
    "# Compute percentage\n",
    "connected_percentage = (connected_dishes / total_dishes) * 100\n",
    "unconnected_percentage = 100 - connected_percentage\n",
    "\n",
    "print(f\"✅ Connected Dishes: {connected_dishes}/{total_dishes} ({connected_percentage:.2f}%)\")\n",
    "print(f\"⚠️ Unconnected Dishes: {total_dishes - connected_dishes}/{total_dishes} ({unconnected_percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3df1c1ff-c3b7-4ebc-aec6-8fa3d0ff69e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Edge Index Sorted?: False\n"
     ]
    }
   ],
   "source": [
    "# Check if edge_index is sorted\n",
    "edge_index = data[\"user\", \"interacts\", \"dish\"].edge_index\n",
    "is_sorted = torch.all(edge_index[1][:-1] <= edge_index[1][1:])\n",
    "\n",
    "print(f\"Is Edge Index Sorted?: {is_sorted.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d40834-8dde-4a25-bea0-3f274601c7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
